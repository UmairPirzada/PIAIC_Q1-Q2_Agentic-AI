{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmairPirzada/PIAIC_Q1-Q2_Agentic-AI/blob/main/Function_calling_config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4DhA4907Asz"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU-mY9hi8pQh"
      },
      "source": [
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp3W4Pdf8rBO"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=userdata.get(\"GOOGLE_GENAI_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJqil-VL8ug-"
      },
      "source": [
        "## Set up a model with tools\n",
        "\n",
        "This example uses 3 functions that control a simple hypothetical lighting system. Using these functions requires them to be called in a specific order. For example, you must turn the light system on before you can change color.\n",
        "\n",
        "While you can pass these directly to the model and let it try to call them correctly, specifying the `function_calling_config` gives you precise control over the functions that are available to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLS26n7A9l9B"
      },
      "outputs": [],
      "source": [
        "def enable_lights():\n",
        "    \"\"\"Turn on the lighting system.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights enabled.\")\n",
        "\n",
        "\n",
        "def set_light_color(rgb_hex: str):\n",
        "    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"\n",
        "    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n",
        "\n",
        "\n",
        "def stop_lights():\n",
        "    \"\"\"Stop flashing lights.\"\"\"\n",
        "    print(\"LIGHTBOT: Lights turned off.\")\n",
        "\n",
        "\n",
        "light_controls = [enable_lights, set_light_color, stop_lights]\n",
        "instruction = \"You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks.\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    \"models/gemini-1.5-pro\", tools=light_controls, system_instruction=instruction\n",
        ")\n",
        "\n",
        "chat = model.start_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqROCznFCj_Y"
      },
      "source": [
        "Create a helper function for setting `function_calling_config` on `tool_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QgLFPL4Chon"
      },
      "outputs": [],
      "source": [
        "from google.generativeai.types import content_types\n",
        "from collections.abc import Iterable\n",
        "\n",
        "\n",
        "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
        "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
        "    return content_types.to_tool_config(\n",
        "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMEuh_MFdMf"
      },
      "source": [
        "## Text-only mode: `NONE`\n",
        "\n",
        "If you have provided the model with tools, but do not want to use those tools for the current conversational turn, then specify `NONE` as the mode. `NONE` tells the model not to make any function calls, and will behave as though none have been provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZlIFwXqGA09",
        "outputId": "b319a601-03c3-4006-e62b-9b68eecce945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2001.09ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can turn lights on and off, and I can set the color of the lights.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"none\")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Hello light-bot, what can you do?\", tool_config=tool_config\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uux063sjHZ_Z"
      },
      "source": [
        "## Automatic mode: `AUTO`\n",
        "\n",
        "To allow the model to decide whether to respond in text or call specific functions, you can specify `AUTO` as the mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwO9dUjvHoT8",
        "outputId": "b23470f4-535f-40d2-e789-eaee9c466e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 4855.21ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"enable_lights\"\n",
            "  args {\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"auto\")\n",
        "\n",
        "response = chat.send_message(\"Light this place up!\", tool_config=tool_config)\n",
        "print(response.parts[0])\n",
        "chat.rewind();  # You are not actually calling the function, so remove this from the history."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "v683540jAG9i",
        "outputId": "26e8743d-954b-4c48-9953-24cb0e18c314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"function_call\": {\n",
              "                  \"name\": \"enable_lights\",\n",
              "                  \"args\": {}\n",
              "                }\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -1.986752901454262e-07\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 171,\n",
              "        \"candidates_token_count\": 3,\n",
              "        \"total_token_count\": 174\n",
              "      },\n",
              "      \"model_version\": \"gemini-1.5-pro-002\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.parts[0]"
      ],
      "metadata": {
        "id": "_VENcQiFAYEh",
        "outputId": "50485b41-ea11-416f-f3f4-ce8f6eab1096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "function_call {\n",
              "  name: \"enable_lights\"\n",
              "  args {\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHhaO-P9CBPb"
      },
      "source": [
        "## Function-calling mode: `ANY`\n",
        "\n",
        "Setting the mode to `ANY` will force the model to make a function call. By setting `allowed_function_names`, the model will only choose from those functions. If it is not set, all of the functions in `tools` are candidates for function calling.\n",
        "\n",
        "In this example system, if the lights are already on, then the user can change color or turn the lights off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQpz94zrCNJF",
        "outputId": "a9cb7156-0a56-4b75-ac10-58130321e18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1771.23ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 785.90ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 785.13ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"set_light_color\"\n",
            "  args {\n",
            "    fields {\n",
            "      key: \"rgb_hex\"\n",
            "      value {\n",
            "        string_value: \"FF00FF\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "available_fns = [\"set_light_color\", \"stop_lights\"]\n",
        "\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "response = chat.send_message(\"Make this place PURPLE!\", tool_config=tool_config)\n",
        "print(response.parts[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "kPeqscHaBfZg",
        "outputId": "615df629-b65b-4e01-8fb9-c3b1e47548de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"function_call\": {\n",
            "                  \"name\": \"set_light_color\",\n",
            "                  \"args\": {\n",
            "                    \"rgb_hex\": \"FF00FF\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.00021397739571208754\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 172,\n",
            "        \"candidates_token_count\": 12,\n",
            "        \"total_token_count\": 184\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-pro-002\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.parts)"
      ],
      "metadata": {
        "id": "jsJtCdV8EOMg",
        "outputId": "8e12ba8e-61b9-4674-93c4-bd26b5b42550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[function_call {\n",
            "  name: \"set_light_color\"\n",
            "  args {\n",
            "    fields {\n",
            "      key: \"rgb_hex\"\n",
            "      value {\n",
            "        string_value: \"FF00FF\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "available_fns = [\"set_light_color\", \"stop_lights\"]\n",
        "\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "response = chat.send_message(\"Make this place Blue!\", tool_config=tool_config)\n",
        "print(response.parts[0])"
      ],
      "metadata": {
        "id": "O7rTgb7qE3e6",
        "outputId": "e6988fea-3f85-4f1a-ae04-f1952bfd2a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"set_light_color\"\n",
            "  args {\n",
            "    fields {\n",
            "      key: \"rgb_hex\"\n",
            "      value {\n",
            "        string_value: \"0000FF\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cGrRy-uJ7-J"
      },
      "source": [
        "## Automatic function calling\n",
        "\n",
        "`tool_config` works when enabling automatic function calling too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx7aIX8OXvi6",
        "outputId": "866a8370-17a6-4a2e-c148-5b45bc2db11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights enabled.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Lights enabled.\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.525895873705546\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 169,\n",
              "        \"candidates_token_count\": 3,\n",
              "        \"total_token_count\": 172\n",
              "      },\n",
              "      \"model_version\": \"gemini-1.5-pro-002\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "available_fns = [\"enable_lights\"]\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "auto_chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "response = auto_chat.send_message(\"It's awful dark in here...\", tool_config=tool_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "cC-1_UBZFltS",
        "outputId": "049a80f3-ce63-4b61-9efe-9a5fa701bf27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"function_call\": {\n",
              "                  \"name\": \"set_light_color\",\n",
              "                  \"args\": {\n",
              "                    \"rgb_hex\": \"0000FF\"\n",
              "                  }\n",
              "                }\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.007631147710176615\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 201,\n",
              "        \"candidates_token_count\": 13,\n",
              "        \"total_token_count\": 214\n",
              "      },\n",
              "      \"model_version\": \"gemini-1.5-pro-002\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_light_values(brightness, color_temp):\n",
        "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
        "\n",
        "    Args:\n",
        "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
        "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the set brightness and color temperature.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"brightness\": brightness,\n",
        "        \"colorTemperature\": color_temp\n",
        "    }"
      ],
      "metadata": {
        "id": "cml9sWtJJDm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
        "                              tools=[set_light_values])"
      ],
      "metadata": {
        "id": "8UgJBGwFJ5Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal"
      ],
      "metadata": {
        "id": "l-M8wxtXKs8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def power_disco_ball(power: bool) -> bool:\n",
        "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
        "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
        "    \"\"\"Play some music matching the specified parameters.\n",
        "\n",
        "    Args:\n",
        "      energetic: Whether the music is energetic or not.\n",
        "      loud: Whether the music is loud or not.\n",
        "      bpm: The beats per minute of the music.\n",
        "\n",
        "    Returns: The name of the song being played.\n",
        "    \"\"\"\n",
        "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
        "    return \"Never gonna give you up.\"\n",
        "\n",
        "\n",
        "def dim_lights(brightness: float) -> bool:\n",
        "    \"\"\"Dim the lights.\n",
        "\n",
        "    Args:\n",
        "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
        "    \"\"\"\n",
        "    print(f\"Lights are now set to {brightness:.0%}\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "DRfwkLatL5Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model up with tools.\n",
        "house_fns = [power_disco_ball, start_music, dim_lights]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
        "\n",
        "# Call the API.\n",
        "chat = model.start_chat()\n",
        "response = chat.send_message(\"Turn this place into a party!\")\n",
        "\n",
        "# Print out each of the function calls requested from this single call.\n",
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "id": "DIwU6JvzMaPU",
        "outputId": "44e971d8-daab-4a2b-c5fe-50b87d45bd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1948.31ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1466.43ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1013.47ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 685.66ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1088.14ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 658.68ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1350.06ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 760.12ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "power_disco_ball(power=True)\n",
            "start_music(loud=True, energetic=True, bpm=120.0)\n",
            "dim_lights(brightness=0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7E2UBr0yMi6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the responses from the specified tools.\n",
        "responses = {\n",
        "    \"power_disco_ball\": True,\n",
        "    \"start_music\": \"Never gonna give you up.\",\n",
        "    \"dim_lights\": True,\n",
        "}\n",
        "\n",
        "# Build the response parts.\n",
        "response_parts = [\n",
        "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
        "    for fn, val in responses.items()\n",
        "]\n",
        "\n",
        "response = chat.send_message(response_parts)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "wNakMdVdMeSL",
        "outputId": "60512ce0-7644-4d83-ad5b-f1488e0580af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ok, I've turned on the disco ball, started playing \"Never gonna give you up\" at 120bpm, and dimmed the lights to 50% brightness.  Let's get this party started!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a:float, b:float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.5-flash',\n",
        "                             tools=[multiply])\n",
        "\n",
        "model._tools.to_proto()"
      ],
      "metadata": {
        "id": "X0bFnHkjMlRj",
        "outputId": "d3a56d50-118a-4a7a-e120-c156a57c8771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_declarations {\n",
              "   name: \"multiply\"\n",
              "   description: \"returns a * b.\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"b\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"a\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     required: \"a\"\n",
              "     required: \"b\"\n",
              "   }\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model._tools.to_proto()"
      ],
      "metadata": {
        "id": "KPkduajENrH_",
        "outputId": "693e2ae6-2e16-49f0-86b2-4553dfe44e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_declarations {\n",
              "   name: \"multiply\"\n",
              "   description: \"Returns the product of two numbers.\"\n",
              "   parameters {\n",
              "     type_: OBJECT\n",
              "     properties {\n",
              "       key: \"b\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     properties {\n",
              "       key: \"a\"\n",
              "       value {\n",
              "         type_: NUMBER\n",
              "       }\n",
              "     }\n",
              "     required: \"a\"\n",
              "     required: \"b\"\n",
              "   }\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculator = genai.protos.Tool(\n",
        "    function_declarations=[\n",
        "      genai.protos.FunctionDeclaration(\n",
        "        name='multiply',\n",
        "        description=\"Returns the product of two numbers.\",\n",
        "        parameters=genai.protos.Schema(\n",
        "            type=genai.protos.Type.OBJECT,\n",
        "            properties={\n",
        "                'a':genai.protos.Schema(type=genai.protos.Type.NUMBER),\n",
        "                'b':genai.protos.Schema(type=genai.protos.Type.NUMBER)\n",
        "            },\n",
        "            required=['a','b']\n",
        "        )\n",
        "      )\n",
        "    ])"
      ],
      "metadata": {
        "id": "HJXa0UGVMuFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculator = {'function_declarations': [\n",
        "      {'name': 'multiply',\n",
        "       'description': 'Returns the product of two numbers.',\n",
        "       'parameters': {'type_': 'OBJECT',\n",
        "       'properties': {\n",
        "         'a': {'type_': 'NUMBER'},\n",
        "         'b': {'type_': 'NUMBER'} },\n",
        "       'required': ['a', 'b']} }]}"
      ],
      "metadata": {
        "id": "vybfDr8wMwv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash', tools=calculator)\n",
        "chat = model.start_chat()\n",
        "\n",
        "response = chat.send_message(\n",
        "    f\"What's 234551 X 325552 ?\",\n",
        ")"
      ],
      "metadata": {
        "id": "-xFm-H3iM0Uh",
        "outputId": "8cc29f67-404b-4445-d554-52256a887bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2226.63ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates"
      ],
      "metadata": {
        "id": "tf-YrRRxM6F4",
        "outputId": "6c9eeda1-5228-4a37-d4f9-873156524c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[content {\n",
              "  parts {\n",
              "    function_call {\n",
              "      name: \"multiply\"\n",
              "      args {\n",
              "        fields {\n",
              "          key: \"b\"\n",
              "          value {\n",
              "            number_value: 325552\n",
              "          }\n",
              "        }\n",
              "        fields {\n",
              "          key: \"a\"\n",
              "          value {\n",
              "            number_value: 234551\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "avg_logprobs: -0.00061810257223745191\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = response.candidates[0].content.parts[0].function_call\n",
        "assert fc.name == 'multiply'\n",
        "\n",
        "result = fc.args['a'] * fc.args['b']\n",
        "result"
      ],
      "metadata": {
        "id": "xvTlSSg-M9rK",
        "outputId": "e5a3eb07-8e96-440b-f658-5754144b5c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76358547152.0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    genai.protos.Content(\n",
        "    parts=[genai.protos.Part(\n",
        "        function_response = genai.protos.FunctionResponse(\n",
        "          name='multiply',\n",
        "          response={'result': result}))]))"
      ],
      "metadata": {
        "id": "EeFJM_W0NAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "oqaePI2EN-Z2",
        "outputId": "5726341b-b1d2-4263-d2ea-f2b6cd679225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"234551 multiplied by 325552 is 76,358,547,152.\\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.025572675647157612\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 121,\n",
              "        \"candidates_token_count\": 33,\n",
              "        \"total_token_count\": 154\n",
              "      },\n",
              "      \"model_version\": \"gemini-1.5-flash\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}